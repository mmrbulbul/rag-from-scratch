"""Quality checker of the questions generated by qa generator"""

from tqdm import tqdm
from rag_systems.utils.utils import call_llm, create_prompt
from rag_systems.utils.prompt_templates import (
    question_standalone_critique_prompt,
    question_groundedness_critique_prompt,
    question_relevance_critique_prompt
)


class QAValidator:
    def __init__(self):
        pass

    def validate(self, llm_client, outputs):
        print("Generating critique for each QA couple...")

        for output in tqdm(outputs):
            evaluations = {
                "groundedness": call_llm(
                    llm_client,
                    create_prompt(
                        question_groundedness_critique_prompt,
                        context=output["context"],
                        question=output["question"]),
                ),
                "relevance": call_llm(
                    llm_client,
                    create_prompt(
                        question_relevance_critique_prompt,
                        question=output["question"]),
                ),
                "standalone": call_llm(
                    llm_client,
                    create_prompt(
                        question_standalone_critique_prompt,
                        question=output["question"]),
                ),
            }
            try:
                for criterion, evaluation in evaluations.items():
                    score, eval = (
                        int(evaluation.split("Total rating: ")[-1].strip()),
                        evaluation.split(
                            "Total rating: ")[-2].split("Evaluation: ")[1],
                    )
                    output.update(
                        {
                            f"{criterion}_score": score,
                            f"{criterion}_eval": eval,
                        }
                    )
            except Exception as e:
                continue
